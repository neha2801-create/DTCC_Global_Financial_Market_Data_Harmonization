{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Global Financial Market Data Harmonization Project\n",
        "\n",
        "## Problem Statement\n",
        "Despite post-2008 financial crisis reforms in derivatives trading and reporting, financial regulators face significant challenges in obtaining a comprehensive view of market activity due to:\n",
        "- Inconsistent data formats across different jurisdictions\n",
        "- Varying reporting standards between exchanges\n",
        "- Multiple currency denominations\n",
        "- Lack of standardized data elements\n",
        "- Difficulties in cross-border data sharing and analysis\n",
        "\n",
        "## Objective\n",
        "Develop a machine learning solution to:\n",
        "1. Harmonize financial market data across different exchanges\n",
        "2. Create standardized reporting formats\n",
        "3. Enable real-time risk assessment across global markets\n",
        "4. Facilitate regulatory oversight through unified data analysis"
      ],
      "metadata": {
        "id": "Pw50PaQOHphn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. RSI → Momentum and Reversal Points\n",
        "2. MACD → Trend Changes and Strength\n",
        "3. Volume → Confirmation of Price Moves\n",
        "4. Volatility → Risk Levels\n",
        "5. Bollinger Bands → Price Channels and Extremes"
      ],
      "metadata": {
        "id": "b05DWqM5WVWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras tensorflow --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv7GkUGDJDeC",
        "outputId": "c1b83f7d-b1dc-4ef0-c4bc-9c3caeb5104d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "DFysyZpuHsIC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# ML models and preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class MarketAnalyzer:\n",
        "    def __init__(self, csv_path):\n",
        "        \"\"\"Initialize with CSV file path\"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.df = None\n",
        "        self.models = {}\n",
        "        self.predictions = {}\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and prepare CSV data\"\"\"\n",
        "        try:\n",
        "            # Read CSV file\n",
        "            self.df = pd.read_csv(self.csv_path)\n",
        "            print(f\"Successfully loaded data with shape: {self.df.shape}\")\n",
        "            print(\"\\nColumns found:\", self.df.columns.tolist())\n",
        "\n",
        "            # Convert Date column\n",
        "            if 'Date' in self.df.columns:\n",
        "                self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
        "            else:\n",
        "                # Try to find date column with different case\n",
        "                date_col = [col for col in self.df.columns if col.lower() == 'date']\n",
        "                if date_col:\n",
        "                    self.df['Date'] = pd.to_datetime(self.df[date_col[0]])\n",
        "                    self.df = self.df.rename(columns={date_col[0]: 'Date'})\n",
        "\n",
        "            # Sort by date\n",
        "            self.df = self.df.sort_values('Date')\n",
        "            print(\"Data sorted by date.\")\n",
        "            return self.df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading CSV file: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def create_features(self):\n",
        "        \"\"\"Create technical indicators for analysis\"\"\"\n",
        "        df = self.df.copy()\n",
        "\n",
        "        # Basic price features\n",
        "        df['Returns'] = df['Close'].pct_change()\n",
        "        df['Log_Returns'] = np.log(df['Close']/df['Close'].shift(1))\n",
        "\n",
        "        # Moving averages\n",
        "        for window in [5, 20, 50]:\n",
        "            df[f'MA{window}'] = df['Close'].rolling(window=window).mean()\n",
        "\n",
        "        # Volatility\n",
        "        df['Volatility'] = df['Returns'].rolling(window=20).std()\n",
        "\n",
        "        # RSI\n",
        "        delta = df['Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # MACD\n",
        "        exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "        exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "        df['MACD'] = exp1 - exp2\n",
        "\n",
        "        # Bollinger Bands\n",
        "        df['BB_middle'] = df['Close'].rolling(window=20).mean()\n",
        "        df['BB_upper'] = df['BB_middle'] + 2*df['Close'].rolling(window=20).std()\n",
        "        df['BB_lower'] = df['BB_middle'] - 2*df['Close'].rolling(window=20).std()\n",
        "\n",
        "        # Volume features\n",
        "        df['Volume_MA'] = df['Volume'].rolling(window=20).mean()\n",
        "        df['Volume_Rate'] = df['Volume']/df['Volume_MA']\n",
        "\n",
        "        # Drop NaN values\n",
        "        self.df = df.dropna()\n",
        "\n",
        "        print(f\"Features created. Data now has {self.df.shape[0]} rows and {self.df.shape[1]} columns.\")\n",
        "        print(self.df.head())  # Inspecting the data after feature creation\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def prepare_sequences(self, sequence_length=10):\n",
        "        \"\"\"Prepare sequences for LSTM\"\"\"\n",
        "        features = ['Returns', 'Log_Returns', 'MA5', 'MA20', 'MA50',\n",
        "                    'Volatility', 'RSI', 'MACD', 'Volume_Rate']\n",
        "\n",
        "        # Scale features\n",
        "        scaled_data = self.scaler.fit_transform(self.df[features])\n",
        "        print(f\"Scaled data shape: {scaled_data.shape}\")\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for i in range(len(scaled_data) - sequence_length):\n",
        "            X.append(scaled_data[i:(i + sequence_length)])\n",
        "            y.append(self.df['Returns'].values[i + sequence_length])\n",
        "\n",
        "        X, y = np.array(X), np.array(y)\n",
        "        print(f\"Prepared {X.shape[0]} sequences for LSTM model.\")\n",
        "        return X, y\n",
        "\n",
        "    def build_lstm_model(self, input_shape):\n",
        "        \"\"\"Build LSTM model\"\"\"\n",
        "        model = Sequential([\n",
        "            LSTM(100, return_sequences=True, input_shape=input_shape),\n",
        "            Dropout(0.2),\n",
        "            LSTM(50, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(25),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "        return model\n",
        "\n",
        "    def train_models(self, X_lstm, y):\n",
        "        \"\"\"Train multiple ML models\"\"\"\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_lstm, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "        # Train LSTM\n",
        "        lstm_model = self.build_lstm_model(X_lstm.shape[1:])\n",
        "        lstm_history = lstm_model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            validation_split=0.1,\n",
        "            verbose=1\n",
        "        )\n",
        "        self.models['LSTM'] = lstm_model\n",
        "        self.predictions['LSTM'] = lstm_model.predict(X_test)\n",
        "\n",
        "        # Train traditional ML models\n",
        "        features = ['Returns', 'Log_Returns', 'MA5', 'MA20', 'Volatility', 'RSI', 'MACD']\n",
        "        X_trad = self.df[features].values[len(X_lstm):]\n",
        "        y_trad = y\n",
        "\n",
        "        models_config = {\n",
        "            'RF': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'XGB': XGBRegressor(n_estimators=100, learning_rate=0.1),\n",
        "            'SVR': SVR(kernel='rbf'),\n",
        "            'GB': GradientBoostingRegressor(n_estimators=100)\n",
        "        }\n",
        "\n",
        "        for name, model in models_config.items():\n",
        "            model.fit(X_trad[:-len(X_test)], y_trad[:-len(X_test)])\n",
        "            self.models[name] = model\n",
        "            self.predictions[name] = model.predict(X_trad[-len(X_test):])\n",
        "\n",
        "        return y_test, lstm_history\n",
        "\n",
        "    def detect_anomalies(self):\n",
        "        \"\"\"Detect market anomalies\"\"\"\n",
        "        features = ['Returns', 'Volatility', 'Volume_Rate']\n",
        "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "        return iso_forest.fit_predict(self.df[features]) == -1\n",
        "\n",
        "    def plot_results(self, y_test, anomalies, history):\n",
        "        \"\"\"Create visualizations\"\"\"\n",
        "        plt.style.use('seaborn')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # Model predictions\n",
        "        ax1 = plt.subplot(321)\n",
        "        ax1.plot(y_test, label='Actual', color='black')\n",
        "        for name, pred in self.predictions.items():\n",
        "            ax1.plot(pred, label=name, alpha=0.7)\n",
        "        ax1.set_title('Model Predictions Comparison')\n",
        "        ax1.legend()\n",
        "\n",
        "        # Performance metrics\n",
        "        ax2 = plt.subplot(322)\n",
        "        metrics = {name: {\n",
        "            'MSE': mean_squared_error(y_test, pred),\n",
        "            'R2': r2_score(y_test, pred)\n",
        "        } for name, pred in self.predictions.items()}\n",
        "\n",
        "        model_names = list(metrics.keys())\n",
        "        mse_scores = [m['MSE'] for m in metrics.values()]\n",
        "        r2_scores = [m['R2'] for m in metrics.values()]\n",
        "\n",
        "        x = np.arange(len(model_names))\n",
        "        width = 0.35\n",
        "        ax2.bar(x - width/2, mse_scores, width, label='MSE')\n",
        "        ax2.bar(x + width/2, r2_scores, width, label='R2')\n",
        "        ax2.set_xticks(x)\n",
        "        ax2.set_xticklabels(model_names, rotation=45)\n",
        "        ax2.set_title('Model Performance Metrics')\n",
        "        ax2.legend()\n",
        "\n",
        "        # Anomaly detection\n",
        "        ax3 = plt.subplot(323)\n",
        "        ax3.scatter(self.df.index[anomalies],\n",
        "                   self.df['Close'][anomalies],\n",
        "                   color='red',\n",
        "                   label='Anomaly')\n",
        "        ax3.plot(self.df.index, self.df['Close'],\n",
        "                color='blue',\n",
        "                alpha=0.7,\n",
        "                label='Price')\n",
        "        ax3.set_title('Anomaly Detection')\n",
        "        ax3.legend()\n",
        "\n",
        "        # Feature correlations\n",
        "        ax4 = plt.subplot(324)\n",
        "        features = ['Returns', 'Volatility', 'RSI', 'MACD', 'Volume_Rate']\n",
        "        sns.heatmap(self.df[features].corr(), annot=True, cmap='coolwarm', ax=ax4)\n",
        "        ax4.set_title('Feature Correlations')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Example of calling plot_results after model training\n",
        "def main(csv_path):\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(f\"Starting analysis of {csv_path}\")\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = MarketAnalyzer(csv_path)\n",
        "\n",
        "    # Load and process data\n",
        "    print(\"Loading data...\")\n",
        "    analyzer.load_data()\n",
        "\n",
        "    print(\"Creating features...\")\n",
        "    analyzer.create_features()\n",
        "\n",
        "    # Prepare sequences for LSTM model\n",
        "    X_lstm, y = analyzer.prepare_sequences()\n",
        "\n",
        "    # Train models\n",
        "    y_test, lstm_history = analyzer.train_models(X_lstm, y)\n",
        "\n",
        "    # Detect anomalies\n",
        "    anomalies = analyzer.detect_anomalies()\n",
        "\n",
        "    # Plot results after model training\n",
        "    analyzer.plot_results(y_test, anomalies, lstm_history)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify your CSV file path\n",
        "    csv_file = \"/indexProcessed.csv\"  # Replace with your CSV file path\n",
        "\n",
        "    # Run analysis\n",
        "    main(csv_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify your CSV file path\n",
        "    csv_file = \"/indexProcessed.csv\"  # Replace with your CSV file path\n",
        "\n",
        "    # Run analysis\n",
        "    analyzer = main(csv_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ZsbEw3Ma17E",
        "outputId": "a7ac0f30-4229-42e4-a49a-5e23fb72052e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting analysis of /indexProcessed.csv\n",
            "Loading data...\n",
            "Successfully loaded data with shape: (104224, 9)\n",
            "\n",
            "Columns found: ['Index', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'CloseUSD']\n",
            "Data sorted by date.\n",
            "Creating features...\n",
            "Features created. Data now has 89866 rows and 22 columns.\n",
            "        Index       Date         Open         High          Low        Close  \\\n",
            "25896    IXIC 1984-10-11   244.199997   244.699997   244.199997   244.699997   \n",
            "13206     NYA 1984-10-11   993.609985   993.609985   993.609985   993.609985   \n",
            "25897    IXIC 1984-10-12   245.500000   246.199997   245.500000   246.199997   \n",
            "13207     NYA 1984-10-12  1002.280029  1002.280029  1002.280029  1002.280029   \n",
            "67345  GSPTSE 1984-10-12  2370.600098  2388.100098  2370.600098  2386.699951   \n",
            "\n",
            "         Adj Close      Volume     CloseUSD   Returns  ...         MA20  \\\n",
            "25896   244.699997  62860000.0   244.699997 -0.896572  ...  3464.469513   \n",
            "13206   993.609985         0.0   993.609985  3.060523  ...  3464.480011   \n",
            "25897   246.199997  58860000.0   246.199997 -0.752217  ...  2944.686495   \n",
            "13207  1002.280029         0.0  1002.280029  3.070999  ...  2982.540497   \n",
            "67345  2380.638916         0.0  1980.960959  1.381271  ...  2983.560492   \n",
            "\n",
            "              MA50  Volatility        RSI        MACD    BB_middle  \\\n",
            "25896  3448.393595    9.840104  49.446645 -263.767117  3464.469513   \n",
            "13206  3448.095394    9.777760  50.553401 -403.853872  3464.480011   \n",
            "25897  3432.829995    9.765351  40.994267 -568.628578  2944.686495   \n",
            "13207  3240.778799    9.698522  50.785441 -630.931396  2982.540497   \n",
            "67345  3241.182798    9.659124  51.429376 -562.115987  2983.560492   \n",
            "\n",
            "           BB_upper     BB_lower  Volume_MA  Volume_Rate  \n",
            "25896  12149.802655 -5220.863629  3143000.0    20.000000  \n",
            "13206  12149.800577 -5220.840555  3143000.0     0.000000  \n",
            "25897  11046.032532 -5156.659541  6086000.0     9.671377  \n",
            "13207  11037.720499 -5072.639506  6086000.0     0.000000  \n",
            "67345  11038.417098 -5071.296114  6086000.0     0.000000  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "Scaled data shape: (89866, 9)\n",
            "Prepared 89856 sequences for LSTM model.\n",
            "Epoch 1/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 25ms/step - loss: 35.1492 - mae: 2.7653 - val_loss: 25.0805 - val_mae: 2.0733\n",
            "Epoch 2/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 21ms/step - loss: 25.4875 - mae: 2.1724 - val_loss: 22.8050 - val_mae: 2.2271\n",
            "Epoch 3/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 22ms/step - loss: 23.2599 - mae: 2.0628 - val_loss: 21.7998 - val_mae: 1.7476\n",
            "Epoch 4/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21ms/step - loss: 22.6972 - mae: 1.9774 - val_loss: 21.8335 - val_mae: 1.8996\n",
            "Epoch 5/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 22ms/step - loss: 22.5054 - mae: 1.9760 - val_loss: 20.4511 - val_mae: 1.7768\n",
            "Epoch 6/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 20ms/step - loss: 22.4736 - mae: 1.9746 - val_loss: 20.9774 - val_mae: 1.7835\n",
            "Epoch 7/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 23ms/step - loss: 21.7500 - mae: 1.9426 - val_loss: 20.4268 - val_mae: 1.9341\n",
            "Epoch 8/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 23ms/step - loss: 23.0626 - mae: 1.9577 - val_loss: 20.0117 - val_mae: 1.9061\n",
            "Epoch 9/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 20ms/step - loss: 22.7206 - mae: 1.9692 - val_loss: 19.5554 - val_mae: 1.8491\n",
            "Epoch 10/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 22ms/step - loss: 22.1457 - mae: 1.9357 - val_loss: 19.4697 - val_mae: 1.7450\n",
            "Epoch 11/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22ms/step - loss: 23.1159 - mae: 1.9922 - val_loss: 19.2846 - val_mae: 1.6716\n",
            "Epoch 12/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 23ms/step - loss: 23.3836 - mae: 1.9786 - val_loss: 19.5909 - val_mae: 1.8534\n",
            "Epoch 13/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 21ms/step - loss: 21.9791 - mae: 1.9460 - val_loss: 18.8535 - val_mae: 1.7124\n",
            "Epoch 14/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 23ms/step - loss: 21.8935 - mae: 1.9482 - val_loss: 20.0293 - val_mae: 1.6630\n",
            "Epoch 15/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 23ms/step - loss: 22.2362 - mae: 1.9558 - val_loss: 19.0252 - val_mae: 1.8324\n",
            "Epoch 16/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 22ms/step - loss: 21.7597 - mae: 1.9429 - val_loss: 18.7951 - val_mae: 1.7492\n",
            "Epoch 17/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21ms/step - loss: 21.2543 - mae: 1.9062 - val_loss: 19.9143 - val_mae: 2.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 24ms/step - loss: 21.8008 - mae: 1.9602 - val_loss: 19.5803 - val_mae: 1.7800\n",
            "Epoch 19/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 21ms/step - loss: 21.1225 - mae: 1.9196 - val_loss: 19.4597 - val_mae: 1.8021\n",
            "Epoch 20/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 21ms/step - loss: 21.0825 - mae: 1.9202 - val_loss: 18.9357 - val_mae: 1.7151\n",
            "Epoch 21/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 21ms/step - loss: 20.4360 - mae: 1.8942 - val_loss: 20.9256 - val_mae: 1.9988\n",
            "Epoch 22/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 24ms/step - loss: 20.7736 - mae: 1.9145 - val_loss: 19.2644 - val_mae: 1.6666\n",
            "Epoch 23/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 23ms/step - loss: 22.0417 - mae: 1.9367 - val_loss: 19.8534 - val_mae: 1.7877\n",
            "Epoch 24/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 23ms/step - loss: 20.7915 - mae: 1.9007 - val_loss: 20.0240 - val_mae: 1.8477\n",
            "Epoch 25/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 23ms/step - loss: 20.3835 - mae: 1.8988 - val_loss: 19.5343 - val_mae: 1.8406\n",
            "Epoch 26/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 23ms/step - loss: 21.3295 - mae: 1.9219 - val_loss: 21.4819 - val_mae: 1.9734\n",
            "Epoch 27/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 22ms/step - loss: 20.6278 - mae: 1.9052 - val_loss: 19.5867 - val_mae: 1.8163\n",
            "Epoch 28/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22ms/step - loss: 20.7570 - mae: 1.9197 - val_loss: 19.5293 - val_mae: 1.8141\n",
            "Epoch 29/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 23ms/step - loss: 20.2925 - mae: 1.8893 - val_loss: 20.2878 - val_mae: 1.9087\n",
            "Epoch 30/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 21ms/step - loss: 21.7514 - mae: 1.9405 - val_loss: 19.3121 - val_mae: 1.7525\n",
            "Epoch 31/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 22ms/step - loss: 19.5600 - mae: 1.8493 - val_loss: 19.6023 - val_mae: 1.8345\n",
            "Epoch 32/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22ms/step - loss: 20.3255 - mae: 1.9066 - val_loss: 19.7783 - val_mae: 1.7898\n",
            "Epoch 33/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 24ms/step - loss: 20.7322 - mae: 1.8760 - val_loss: 20.1263 - val_mae: 1.8291\n",
            "Epoch 34/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 22ms/step - loss: 20.2106 - mae: 1.8709 - val_loss: 20.2866 - val_mae: 1.8948\n",
            "Epoch 35/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 22ms/step - loss: 19.8175 - mae: 1.8802 - val_loss: 20.6806 - val_mae: 1.9457\n",
            "Epoch 36/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 22ms/step - loss: 20.9009 - mae: 1.9083 - val_loss: 19.8089 - val_mae: 1.7305\n",
            "Epoch 37/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 22ms/step - loss: 21.0381 - mae: 1.9090 - val_loss: 19.3656 - val_mae: 1.7053\n",
            "Epoch 38/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 23ms/step - loss: 20.0551 - mae: 1.8810 - val_loss: 20.1611 - val_mae: 1.8881\n",
            "Epoch 39/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 21ms/step - loss: 19.9811 - mae: 1.8808 - val_loss: 19.5621 - val_mae: 1.8048\n",
            "Epoch 40/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 23ms/step - loss: 19.0088 - mae: 1.8428 - val_loss: 19.7968 - val_mae: 1.8019\n",
            "Epoch 41/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 22ms/step - loss: 18.0258 - mae: 1.8058 - val_loss: 19.7542 - val_mae: 1.7084\n",
            "Epoch 42/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 22ms/step - loss: 19.5857 - mae: 1.8570 - val_loss: 20.5722 - val_mae: 1.8744\n",
            "Epoch 43/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 22ms/step - loss: 19.0848 - mae: 1.8538 - val_loss: 20.1668 - val_mae: 1.7696\n",
            "Epoch 44/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21ms/step - loss: 18.5962 - mae: 1.8458 - val_loss: 19.9668 - val_mae: 1.6779\n",
            "Epoch 45/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 21ms/step - loss: 19.1547 - mae: 1.8514 - val_loss: 20.1975 - val_mae: 1.7234\n",
            "Epoch 46/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 23ms/step - loss: 17.8675 - mae: 1.8193 - val_loss: 23.2394 - val_mae: 2.1177\n",
            "Epoch 47/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 24ms/step - loss: 18.7822 - mae: 1.8650 - val_loss: 19.9476 - val_mae: 1.7074\n",
            "Epoch 48/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 22ms/step - loss: 18.4581 - mae: 1.8249 - val_loss: 19.8976 - val_mae: 1.7883\n",
            "Epoch 49/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 21ms/step - loss: 18.6273 - mae: 1.8294 - val_loss: 20.3279 - val_mae: 1.8113\n",
            "Epoch 50/50\n",
            "\u001b[1m2022/2022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 21ms/step - loss: 18.9962 - mae: 1.8442 - val_loss: 20.3626 - val_mae: 1.8230\n",
            "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by RandomForestRegressor.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-704703e488da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Run analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-704703e488da>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;31m# Train models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Detect anomalies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-704703e488da>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(self, X_lstm, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 7)) while a minimum of 1 is required by RandomForestRegressor."
          ]
        }
      ]
    }
  ]
}